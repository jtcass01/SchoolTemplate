\title{Chapter: X}

\author{Jacob Taylor Cassady}
\documentclass{article}
\usepackage{textcomp}
\usepackage{textgreek}
\usepackage[dvipsnames]{xcolor}
\usepackage{bbding, pifont}
\usepackage{bbold}
\usepackage{fancyhdr}
\usepackage{background}
\usepackage{dsfont}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{lastpage}
\graphicspath{ {../../Figures/} }

\backgroundsetup{
  scale=1,
  color=black,
  opacity=1,
  angle=0,
  position=current page.south,
  vshift=60pt,
  contents={%
  \small\sffamily%
  \begin{minipage}{.8\textwidth}
%   \parbox[b]{.6\textwidth}{%
%     Page \thepage\ of   \pageref{LastPage}}\hfill
  \textcolor{BlueViolet}{\rule{\textwidth}{1.5pt}}\\
  \end{minipage}\hspace{.02\textwidth}%
  \begin{minipage}{.18\textwidth}
  \includegraphics[width=\linewidth,height=70pt,keepaspectratio]{JHU_LOGO.png}
  \end{minipage}%
  }
}

\pagestyle{empty}

\begin{document}

\maketitle

\section{Exercise 0}
Hello world.

\subsection{Exercise 0.0}
Example Equation - Taken from ``Reinforcement Learning'' by Sutton and Barto:

%  EXAMPLE EQUATIONS
$$ Q_t(a) \doteq \frac{\text{sum of rewards when \emph{a} taken prior to \emph{t}}}{\text{number of times \emph{a} taken prior to \emph{t}}} = \frac{\sum_{i=1}^{t-1} R_i \cdotp \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbb{1}_{A_i=a}} \thickspace \thickspace \thickspace \text{(2.1)} $$
where $ \mathbb{1}_{predicate} $ denotes the random variable that is 1 if \emph{predicate} is true and 0 if it is not.
If the denominator is zero, then we instead define $ Q_t(a) $ as some default value, such as
0. As the denominator goes to infinity, by the law of large numbers, $ Q_t(a) $ converges to
$ q_*(a) $. We call this the \emph{sample-average} method for estimating action values because each
estimate is an average of the sample of relevant rewards.

%  EXAMPLE GRAPHIC
% \begin{figure}[b]
%     \centering
%     \includegraphics{bandit_example_output.png}    
%     \caption{Output of bandit\_example.py}
% \end{figure}

\end{document}
